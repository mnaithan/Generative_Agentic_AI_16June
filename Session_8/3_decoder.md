# Understanding the Decoder: Making Predictions One Word at a Time

## ğŸ§  What Is a Decoder?

While the encoder understands the input sentence (like English), the decoder generates the output sentence (like Hindi).

Just like a translator listens to a sentence and then speaks it out word by word, the decoder works in steps, generating one word at a time.

---

## ğŸ”§ Decoder Block: Step-by-Step Breakdown

Each decoder block has the following components in this order:

| Step | Component                        | What It Does                                        |
| ---- | -------------------------------- | --------------------------------------------------- |
| 1    | Masked Multi-Head Self-Attention | Looks at **previously generated** words only        |
| 2    | Add & Normalize                  | Stabilizes training                                 |
| 3    | Encoderâ€“Decoder Attention        | Focuses on the **input sentence** (encoder output)  |
| 4    | Add & Normalize                  | Again, keeps everything smooth                      |
| 5    | Feed Forward Network             | Adds extra transformation to improve representation |
| 6    | Add & Normalize                  | Final clean-up before passing to next decoder layer |

---

### 1. Masked Multi-Head Self-Attention

**Why â€œmaskedâ€?**

Because the decoder must not peek at future words â€” it can only use the words it has already generated.

Imagine you're writing a translation:

- You've written: â€œà¤®à¥ˆà¤‚ à¤à¤†à¤ˆâ€
- Youâ€™re about to write the next word.
- You canâ€™t look ahead at the correct answer (â€œà¤¸à¥€à¤– à¤°à¤¹à¤¾ à¤¹à¥‚à¤â€) â€” you can only use â€œà¤®à¥ˆà¤‚ à¤à¤†à¤ˆâ€ so far.

ğŸ”‘ **Masking ensures this happens.**

---

### 2. Encoderâ€“Decoder Attention

Now that the decoder knows what it's written so far, it looks at the encoderâ€™s output to get information from the input sentence.

It uses the same Queryâ€“Keyâ€“Value (Q-K-V) mechanism:

- **Query:** decoderâ€™s current word
- **Key & Value:** encoder output vectors

**Example:**  
If youâ€™re generating the word â€œà¤à¤†à¤ˆâ€, the decoder may look back at the word â€œAIâ€ from the input sentence to get help.

---

### 3. Feed Forward + Add & Normalize

This is just like the encoder:

- Each word's vector goes through a small neural network.
- Then we use Add & Norm again to make training stable.

These steps help turn attention outputs into useful representations for the final prediction.

---

## ğŸ§¾ Step-by-Step: How Prediction Happens

1. **Start with Input:**  
   Sentence: â€œTransformers are amazingâ€

2. **Tokenization:**  
   Turns into numbers: [20345, 1012, 8763]

3. **Embeddings + Positional Encoding:**  
   Vectors for each word + position added.

4. **Encoder Processes Input:**  
   Understands relationships (e.g., â€œamazingâ€ describes â€œTransformersâ€).

5. **Decoder Starts Generating Output:**
    - First input: [â€œà¤®à¥ˆà¤‚â€]
    - Predicts next: â€œà¤à¤†à¤ˆâ€, then â€œà¤¸à¥€à¤–â€, then â€œà¤°à¤¹à¤¾â€, then â€œà¤¹à¥‚à¤â€

6. **Each Decoder Layer Includes Attention:**
    - Looks at previous outputs
    - Looks at encoder outputs
    - Combines everything to decide next word

---

## ğŸ¯ Final Output: One Token at a Time

At each step:

- The decoder produces a vector.
- That vector goes through:
    - Linear layer (to match vocab size)
    - Softmax (to get probabilities)

**Example:**

Output vector â†’ {"à¤®à¥ˆà¤‚": 0.6, "à¤¤à¥à¤®": 0.2, "AI": 0.05, ...}  
Highest score â†’ â€œà¤®à¥ˆà¤‚â€ is chosen

The process repeats until a special `[END]` token is generated.

---

## ğŸ§  Recap

| Component               | Role                                                |
|-------------------------|-----------------------------------------------------|
| Masked Self-Attention   | Looks at past words only (no peeking ahead)         |
| Encoderâ€“Decoder Attn.   | Looks at input sentence from encoder                |
| Feed Forward Layer      | Adds more understanding                             |
| Linear + Softmax        | Chooses next word                                   |
| Repeats                 | One word at a time, until done                      |
