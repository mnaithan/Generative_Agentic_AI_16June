{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3iyRl5WQJPE",
        "outputId": "4002e99c-3629-46af-f3e1-aa50c16b0e32"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        }
      ],
      "source": [
        "#Load the text generation pipeline\n",
        "generator = pipeline(\"text-generation\" , model=\"gpt2\")\n",
        "\n",
        "#Generate text\n",
        "prompt = \"I will settle in Himalyas\"\n",
        "output = generator(prompt ,\n",
        "                   max_length=50,\n",
        "                   num_return_sequences = 1,\n",
        "                   do_sample = True,\n",
        "                   top_k = 50,\n",
        "                   top_p = 0.95,\n",
        "                   temperature = 1,\n",
        "                   eos_token_id = 50256)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "transformers.pipelines.text_generation.TextGenerationPipeline"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(generator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'generated_text': 'I will settle in Himalyas, and I will be well and healthy.\" (Exodus 23:28) And they said, \"Then he will take off His garment, and they will be clean,\" and they would have the same result. (Genesis 5:24,25)\\n\\nThe fact that this is the case with women was also well known to the Jews.\\n\\nIn this part of the story of the Exodus, God was not the only one who was to be taken away from women.\\n\\nIn the following book Moses speaks of the \"little woman\" who was being taken from men.\\n\\nIn the beginning there were two sons, one of them was called the Canaanite and one of the other was called the Hebrew. And there was a man called Zerubbabel, who had a man\\'s son, which was called Hebraim and was born of the Canaanite woman.\\n\\nAs the LORD said to Solomon, \"The LORD will remove you from the house of Jacob, and from the land of Egypt.\" (Numbers 13:11)\\n\\nIn the following book of Genesis a certain daughter of Zerubbabel appeared, named Zara, and her name was Zara, or Zoe.\\n\\nZara, or Zoe, came from the name'}]"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBu7Kz0jSEdL",
        "outputId": "0b292e1d-c4bd-4d2d-d499-0f3961aa30aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I will settle in Himalyas, and I will be well and healthy.\" (Exodus 23:28) And they said, \"Then he will take off His garment, and they will be clean,\" and they would have the same result. (Genesis 5:24,25)\n",
            "\n",
            "The fact that this is the case with women was also well known to the Jews.\n",
            "\n",
            "In this part of the story of the Exodus, God was not the only one who was to be taken away from women.\n",
            "\n",
            "In the following book Moses speaks of the \"little woman\" who was being taken from men.\n",
            "\n",
            "In the beginning there were two sons, one of them was called the Canaanite and one of the other was called the Hebrew. And there was a man called Zerubbabel, who had a man's son, which was called Hebraim and was born of the Canaanite woman.\n",
            "\n",
            "As the LORD said to Solomon, \"The LORD will remove you from the house of Jacob, and from the land of Egypt.\" (Numbers 13:11)\n",
            "\n",
            "In the following book of Genesis a certain daughter of Zerubbabel appeared, named Zara, and her name was Zara, or Zoe.\n",
            "\n",
            "Zara, or Zoe, came from the name\n"
          ]
        }
      ],
      "source": [
        "print(output[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYNr3mhSeDVn",
        "outputId": "8f5051a3-d662-4d42-9e10-d57e03244d3e"
      },
      "outputs": [
        {
          "ename": "IndexError",
          "evalue": "list index out of range",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43moutput\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
            "\u001b[1;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ],
      "source": [
        "print(output[1]['generated_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2QGSN5MUGWl"
      },
      "source": [
        "**eos_token_id = 50256**\n",
        "\n",
        "  * End of sequence (stop token) tells the model where to stop generating.\n",
        "  * GPT-2 uses the token-ID 50256 to represent the end of text, if we don't use it it will keep generating the text until the max_length is hit.\n",
        "\n",
        "**do_sample = True**\n",
        "\n",
        "  * It enables the randonmness\n",
        "  * If False => the model always picks the word with the highest probability => very repetitive and boring\n",
        "  * If True => It can explore other likely words.\n",
        "\n",
        "**top_k = 50**\n",
        "\n",
        "    * Controls how many top options to consider when picking the next word\n",
        "    * Pick the next word from top 50 most likely works\n",
        "    * If 1k words are possible next, we narrow it to the top 50 most probable ones, then randomly pick one.\n",
        "\n",
        "**top_p = 0.95**\n",
        "\n",
        "    * Pick from smallest possible set of words whose total probability adds up to 95%\n",
        "\n",
        "**top k is fixed but top_p is flexible based on the context and confidence.**\n",
        "\n",
        "**temperature = 0.7**\n",
        "\n",
        "  * temperature = 1.0 : Normal randomness\n",
        "  * temperature < 1, Less random , more focused\n",
        "  * temperature > 1, More random , more surprising (sometimes may be nonsense)\n",
        "\n",
        "**num_return_sequences = 1**\n",
        "   \n",
        "   * controls number of variations the model should generate for same prompt\n",
        "   * num_return_sequences = 1 -> return only 1 variation.\n",
        "   * num_return_sequences = 3 -> return only 3 different variations of the same prompt.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BK5wPxBZS47z"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "mygenai",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
