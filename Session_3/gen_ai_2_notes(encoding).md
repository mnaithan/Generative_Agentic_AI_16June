# Tokenization & Text Encoding in NLP

---

## üìå What is Tokenization?

**Tokenization** is the process of splitting a sentence into smaller units called **tokens** (usually words or subwords).

**Example:**
- Sentence: `"AI is the future"`
- Tokens: `['AI', 'is', 'the', 'future']`

---

## üìå Why Do We Need Text Encoding?

Machine learning models can‚Äôt work directly with text. We must **convert text into numbers**. That‚Äôs where text encoding techniques like:
- Bag of Words (BoW)
- TF-IDF
- Word Embeddings

come into play.

---

## üî¢ 1. Bag of Words (BoW) Encoding

### ‚úçÔ∏è Example Sentences:

- d‚ÇÅ: `"I am a good Cricket player"` ‚Üí label = 1  
- d‚ÇÇ: `"Rajiv is a bad Chess player"` ‚Üí label = 0

### üî† Vocabulary from both sentences:
`['I', 'am', 'a', 'good', 'Cricket', 'player', 'Rajiv', 'is', 'bad', 'Chess']`

### üßÆ BoW Representation (Binary Encoding):

| Token  | I | am | a | good | Cricket | player | Rajiv | is | bad | Chess | Label |
|--------|---|----|---|------|---------|--------|-------|----|-----|-------|-------|
| d‚ÇÅ     | 1 | 1  | 1 | 1    | 1       | 1      | 0     | 0  | 0   | 0     | 1     |
| d‚ÇÇ     | 0 | 0  | 1 | 0    | 0       | 1      | 1     | 1  | 1   | 1     | 0     |

üìå **Each row becomes a feature vector for machine learning.**

---

### üßë‚Äçüíª BoW Code Example (Manual)

```python
from sklearn.feature_extraction.text import CountVectorizer

docs = [
    "I am a good Cricket player",
    "Rajiv is a bad Chess player"
]

vectorizer = CountVectorizer(binary=True)
bow_matrix = vectorizer.fit_transform(docs)

import pandas as pd
pd.DataFrame(bow_matrix.toarray(), columns=vectorizer.get_feature_names_out())
```

---

## üìä 2. TF-IDF Encoding

**TF-IDF (Term Frequency - Inverse Document Frequency)** is a more refined way to encode text:
- Gives higher weight to unique and important words in a document.
- Down-weights common words (like ‚Äúis‚Äù, ‚Äúa‚Äù, etc.).

### ‚úçÔ∏è TF-IDF Code Example:

```python
from sklearn.feature_extraction.text import TfidfVectorizer

docs = [
    "AI is the future",
    "AI and ML are the future",
    "Physics is really interesting thing",
    "I am interested in concepts of physics"
]

vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform(docs)

import pandas as pd
pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())
```

### üìå Output Table Sample:

|        | ai   | am   | and  | concepts | future | ... |
|--------|------|------|------|----------|--------|-----|
| Doc 1  | 0.70 | 0    | 0    | 0        | 0.70   | ... |
| Doc 2  | 0.50 | 0    | 0.50 | 0        | 0.50   | ... |
| ...    | ...  | ...  | ...  | ...      | ...    | ... |

---

## üß† Key Differences: BoW vs TF-IDF

| Feature         | BoW                    | TF-IDF                            |
|-----------------|-----------------------|-----------------------------------|
| Simplicity      | Very simple           | More sophisticated                |
| Word Weight     | Binary/Count (1 or n) | Based on term importance (float)  |
| Common Words    | Equal importance      | De-emphasized                     |
| Uniqueness      | Ignored               | Boosted                           |

---

## ‚úÖ Summary

| Step             | Description                              |
|------------------|------------------------------------------|
| Tokenization     | Splits text into words                   |
| Integer Encoding | Converts words into unique IDs            |
| BoW              | Simple presence/absence or count of words |
| TF-IDF           | Weighted score showing word importance    |
| Output           | Numerical vectors for ML/DL models        |

---

# Integer Encoding (Word Index Encoding) in NLP

## ‚úÖ What is Integer Encoding?

Integer Encoding is a foundational technique in Natural Language Processing (NLP) where each unique word in a dataset is assigned a unique integer ID.

---

### üìå Purpose

To convert text into numerical format that can be understood and processed by machine learning or deep learning models.

---

## üí¨ Example Input

```python
texts = [
    "Generative AI is intresting",
    "AI is tranforming the world",
    "I want to know about AI more"
]
```
These are raw text sentences ‚Äî in natural language.

---

## üß† Step-by-Step Explanation

### üîπ Step 1: Initialize and Fit the Tokenizer

```python
from tensorflow.keras.preprocessing.text import Tokenizer

tokenizer = Tokenizer()
tokenizer.fit_on_texts(texts)
```

#### What happens here?

- The tokenizer scans all three sentences.
- Builds a word frequency dictionary.
- Assigns unique integers to each word based on frequency.

#### üîç Word Index Generated

```python
print(tokenizer.word_index)
```
**Output:**
```python
{
  'ai': 1, 'is': 2, 'generative': 3, 'intresting': 4,
  'tranforming': 5, 'the': 6, 'world': 7, 'i': 8,
  'want': 9, 'to': 10, 'know': 11, 'about': 12, 'more': 13
}
```
> üß† Note: 'ai' is the most frequent word ‚Üí assigned 1, 'more' is least frequent ‚Üí assigned 13

---

### üîπ Step 2: Convert Texts to Integer Sequences

```python
sequences = tokenizer.texts_to_sequences(texts)
print(sequences)
```

**Output:**
```python
[[3, 1, 2, 4], [1, 2, 5, 6, 7], [8, 9, 10, 11, 12, 1, 13]]
```

| Original Sentence                | Integer Sequence            |
| -------------------------------- | -------------------------- |
| "Generative AI is intresting"    | [3, 1, 2, 4]               |
| "AI is tranforming the world"    | [1, 2, 5, 6, 7]            |
| "I want to know about AI more"   | [8, 9, 10, 11, 12, 1, 13]  |

‚úÖ This numerical format is model-ready for neural networks.

---

### üîπ Step 3: Padding the Sequences

```python
from keras.utils import pad_sequences
padded_sequences = pad_sequences(sequences, padding='post')
```

#### üìå Why Padding?
Neural networks require inputs of the same length. So shorter sequences are padded with 0s.

**Output:**
```python
array([
  [ 3,  1,  2,  4,  0,  0,  0],
  [ 1,  2,  5,  6,  7,  0,  0],
  [ 8,  9, 10, 11, 12,  1, 13]
])
```
> üß† This makes all input vectors uniform in shape.

---

## ‚úÖ What Is This Technique Called?

üëâ **Integer Encoding** (also called Word Indexing)
- NOT one-hot encoding
- NOT Bag of Words
- Used as input for embedding layers or RNN-based models

---

## üß† When & Why Use Integer Encoding?

| Use Case                     | Reason                                    |
| ---------------------------- | ----------------------------------------- |
| Embedding Layer Input        | Embedding layers require integer inputs   |
| Preprocessing before LSTM/GRU| RNNs process sequences, not raw text      |
| Sequence Classification Tasks| Like sentiment analysis, intent detection |
| Token-based Generative AI    | Before feeding tokens to encoder/decoder layers in transformers |

---

## ‚úÖ Real-Life Example: Input to an Embedding Layer

```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Flatten, Dense

model = Sequential()
model.add(Embedding(input_dim=14, output_dim=8, input_length=7))  # 14 words, output vectors of size 8
model.add(Flatten())
model.add(Dense(1, activation='sigmoid'))

model.compile(optimizer='adam', loss='binary_crossentropy')
model.summary()
```

---

## üîö Summary

| Concept           | Explanation                                         |
| ----------------- | -------------------------------------------------- |
| Tokenization      | Splitting text into words                          |
| Integer Encoding  | Assigning each word a unique ID                    |
| Sequence Encoding | Representing text as a sequence of integers        |
| Padding           | Making all sequences the same length               |
| Use in AI Models  | Required for embedding, RNN, LSTM, or transformer inputs |

# üìä Comparison: BoW vs TF-IDF vs Integer Encoding

| **Feature**                   | **Bag of Words (BoW)**                | **TF-IDF (Term Frequency - Inverse Document Frequency)** | **Integer Encoding**                  |
|-------------------------------|---------------------------------------|----------------------------------------------------------|---------------------------------------|
| **Representation**            | Binary or frequency count vectors     | Weighted frequency based on importance                   | Unique integer ID for each word       |
| **Preserves Word Order**      | ‚ùå No                                 | ‚ùå No                                                    | ‚úÖ Yes                                |
| **Vocabulary Dependent**      | ‚úÖ Yes                                | ‚úÖ Yes                                                   | ‚úÖ Yes                                |
| **Handles Word Importance**   | ‚ùå No (all words treated equally)     | ‚úÖ Yes (weights rare/important words higher)              | ‚ùå No (IDs don't reflect importance)  |
| **Output Format**             | Sparse vector (high dimensional)      | Sparse vector (float, high dimensional)                  | Dense list of integers                |
| **Input to Embedding Layer**  | ‚ùå No (must be one-hot or dense)      | ‚ùå No                                                    | ‚úÖ Yes (commonly used)                |
| **Handles Rare Words**        | Poorly (vocab explosion)              | Better (down-weights common words)                       | Poorly unless OOV token is handled    |
| **Interpretable**             | ‚úÖ Yes                                | ‚úÖ Partially (weights can be hard to interpret)           | ‚ùå No (IDs don‚Äôt carry semantic meaning) |
| **Suited For**                | Traditional ML (Naive Bayes, SVM, Logistic) | Traditional ML & feature engineering                 | Deep Learning (RNN, LSTM, Transformer with embedding) |
| **Memory Efficiency**         | ‚ùå Poor (large vocab = huge sparse vectors) | ‚ùå Poor (still sparse and float-heavy)               | ‚úÖ High (compact and integer-based)    |
| **Example**                   | "I love AI" ‚Üí `[1, 0, 1, 0, 1]`      | "I love AI" ‚Üí `[0.7, 0.0, 0.6, 0.0, 0.8]`                | "I love AI" ‚Üí `[2, 5, 1]` <br>*(where {'ai':1, 'i':2, 'love':5})* |


---

## üü¢ Pros & üî¥ Cons of Each Technique

### üßÆ 1. Bag of Words (BoW)

**‚úÖ Pros:**
- Simple and intuitive
- Easy to implement
- Works well for small datasets and traditional models

**‚ùå Cons:**
- Doesn‚Äôt consider word order or context
- High dimensional (sparse vectors)
- Fails to differentiate between important and common words

---

### üìè 2. TF-IDF

**‚úÖ Pros:**
- Adds significance to unique/rare words
- Helps improve classification performance over BoW
- Still compatible with classic ML algorithms

**‚ùå Cons:**
- Doesn‚Äôt handle word order or semantics
- Still results in large, sparse vectors
- Not suitable for deep learning without further transformation

---

### üî¢ 3. Integer Encoding

**‚úÖ Pros:**
- Retains word order
- Compact and efficient for memory
- Perfect for deep learning models using embeddings (LSTM, BERT)

**‚ùå Cons:**
- Integer IDs carry no inherent meaning (1 ‚â† better than 2)
- Needs padding/truncation
- Sensitive to vocabulary and out-of-vocabulary (OOV) issues

---

## ‚úÖ Which to Use When?

| **Use Case**                              | **Recommended Encoding**                 |
|-------------------------------------------|------------------------------------------|
| Simple text classification (Logistic, NB) | BoW or TF-IDF                            |
| Large corpus, need rare word handling     | TF-IDF                                   |
| Deep learning models (LSTM, Transformers) | Integer Encoding + Embedding             |
| Interpretability needed                   | BoW (easiest to explain)                 |

---

## üß† Final Thoughts

- BoW and TF-IDF are good for quick experiments and traditional ML.
- Integer Encoding is essential for deep learning pipelines, especially when combined with word embeddings or transformers.

## üöÄ What‚Äôs Next?

Once text is encoded:
- You can use these vectors for classification, clustering, or embedding into neural networks.
- You can move on to word embeddings like **Word2Vec**, **GloVe**, or contextual models like **BERT**.
